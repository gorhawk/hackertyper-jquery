 static void ucma_put_ctx(struct ucma_context *ctx)\n{\n	if (atomic_dec_and_test(&ctx->ref))\n		complete(&ctx->comp);\n}\n\nstatic struct ucma_context *ucma_alloc_ctx(struct ucma_file *file)\n{\n	struct ucma_context *ctx;\n\n	ctx = kzalloc(sizeof(*ctx), GFP_KERNEL);\n	if (!ctx)\n		return NULL;\n\n	atomic_set(&ctx->ref, 1);\n	init_completion(&ctx->comp);\n	INIT_LIST_HEAD(&ctx->mc_list);\n	ctx->file = file;\n\n	mutex_lock(&mut);\n	ctx->id = idr_alloc(&ctx_idr, ctx, 0, 0, GFP_KERNEL);\n	mutex_unlock(&mut);\n	if (ctx->id < 0)\n		goto error;\n\n	list_add_tail(&ctx->list, &file->ctx_list);\n	return ctx;\n\nerror:\n	kfree(ctx);\n	return NULL;\n}\n\nstatic struct ucma_multicast* ucma_alloc_multicast(struct ucma_context *ctx)\n{\n	struct ucma_multicast *mc;\n\n	mc = kzalloc(sizeof(*mc), GFP_KERNEL);\n	if (!mc)\n		return NULL;\n\n	mutex_lock(&mut);\n	mc->id = idr_alloc(&multicast_idr, mc, 0, 0, GFP_KERNEL);\n	mutex_unlock(&mut);\n	if (mc->id < 0)\n		goto error;\n\n	mc->ctx = ctx;\n	list_add_tail(&mc->list, &ctx->mc_list);\n	return mc;\n\nerror:\n	kfree(mc);\n	return NULL;\n}\n\nstatic void ucma_copy_conn_event(struct rdma_ucm_conn_param *dst,\n				 struct rdma_conn_param *src)\n{\n	if (src->private_data_len)\n		memcpy(dst->private_data, src->private_data,\n		       src->private_data_len);\n	dst->private_data_len = src->private_data_len;\n	dst->responder_resources =src->responder_resources;\n	dst->initiator_depth = src->initiator_depth;\n	dst->flow_control = src->flow_control;\n	dst->retry_count = src->retry_count;\n	dst->rnr_retry_count = src->rnr_retry_count;\n	dst->srq = src->srq;\n	dst->qp_num = src->qp_num;\n}\n\nstatic void ucma_copy_ud_event(struct rdma_ucm_ud_param *dst,\n			       struct rdma_ud_param *src)\n{\n	if (src->private_data_len)\n		memcpy(dst->private_data, src->private_data,\n		       src->private_data_len);\n	dst->private_data_len = src->private_data_len;\n	ib_copy_ah_attr_to_user(&dst->ah_attr, &src->ah_attr);\n	dst->qp_num = src->qp_num;\n	dst->qkey = src->qkey;\n}\n\nstatic void ucma_set_event_context(struct ucma_context *ctx,\n				   struct rdma_cm_event *event,\n				   struct ucma_event *uevent)\n{\n	uevent->ctx = ctx;\n	switch (event->event) {\n	case RDMA_CM_EVENT_MULTICAST_JOIN:\n	case RDMA_CM_EVENT_MULTICAST_ERROR:\n		uevent->mc = (struct ucma_multicast *)\n			     event->param.ud.private_data;\n		uevent->resp.uid = uevent->mc->uid;\n		uevent->resp.id = uevent->mc->id;\n		break;\n	default:\n		uevent->resp.uid = ctx->uid;\n		uevent->resp.id = ctx->id;\n		break;\n	}\n}\n\nstatic int ucma_event_handler(struct rdma_cm_id *cm_id,\n			      struct rdma_cm_event *event)\n{\n	struct ucma_event *uevent;\n	struct ucma_context *ctx = cm_id->context;\n	int ret = 0;\n\n	uevent = kzalloc(sizeof(*uevent), GFP_KERNEL);\n	if (!uevent)\n		return event->event == RDMA_CM_EVENT_CONNECT_REQUEST;\n\n	mutex_lock(&ctx->file->mut);\n	uevent->cm_id = cm_id;\n	ucma_set_event_context(ctx, event, uevent);\n	uevent->resp.event = event->event;\n	uevent->resp.status = event->status;\n	if (cm_id->qp_type == IB_QPT_UD)\n		ucma_copy_ud_event(&uevent->resp.param.ud, &event->param.ud);\n	else\n		ucma_copy_conn_event(&uevent->resp.param.conn,\n				     &event->param.conn);\n\n	if (event->event == RDMA_CM_EVENT_CONNECT_REQUEST) {\n		if (!ctx->backlog) {\n			ret = -ENOMEM;\n			kfree(uevent);\n			goto out;\n		}\n		ctx->backlog--;\n	} else if (!ctx->uid || ctx->cm_id != cm_id) {\n		/*\n		 * We ignore events for new connections until userspace has set\n		 * their context.  This can only happen if an error occurs on a\n		 * new connection before the user accepts it.  This is okay,\n		 * since the accept will just fail later.\n		 */\n		kfree(uevent);\n		goto out;\n	}\n\n	list_add_tail(&uevent->list, &ctx->file->event_list);\n	wake_up_interruptible(&ctx->file->poll_wait);\nout:\n	mutex_unlock(&ctx->file->mut);\n	return ret;\n}\n\nstatic ssize_t ucma_get_event(struct ucma_file *file, const char __user *inbuf,\n			      int in_len, int out_len)\n{\n	struct ucma_context *ctx;\n	struct rdma_ucm_get_event cmd;\n	struct ucma_event *uevent;\n	int ret = 0;\n\n	if (out_len < sizeof uevent->resp)\n		return -ENOSPC;\n\n	if (copy_from_user(&cmd, inbuf, sizeof(cmd)))\n		return -EFAULT;\n\n	mutex_lock(&file->mut);\n	while (list_empty(&file->event_list)) {\n		mutex_unlock(&file->mut);\n\n		if (file->filp->f_flags & O_NONBLOCK)\n			return -EAGAIN;\n\n		if (wait_event_interruptible(file->poll_wait,\n					     !list_empty(&file->event_list)))\n			return -ERESTARTSYS;\n\n		mutex_lock(&file->mut);\n	}\n\n	uevent = list_entry(file->event_list.next, struct ucma_event, list);\n\n	if (uevent->resp.event == RDMA_CM_EVENT_CONNECT_REQUEST) {\n		ctx = ucma_alloc_ctx(file);\n		if (!ctx) {\n			ret = -ENOMEM;\n			goto done;\n		}\n		uevent->ctx->backlog++;\n		ctx->cm_id = uevent->cm_id;\n		ctx->cm_id->context = ctx;\n		uevent->resp.id = ctx->id;\n	}\n\n	if (copy_to_user((void __user *)(unsigned long)cmd.response,\n			 &uevent->resp, sizeof uevent->resp)) {\n		ret = -EFAULT;\n		goto done;\n	}\n\n	list_del(&uevent->list);\n	uevent->ctx->events_reported++;\n	if (uevent->mc)\n		uevent->mc->events_reported++;\n	kfree(uevent);\ndone:\n	mutex_unlock(&file->mut);\n	return ret;\n}\n\nstatic int ucma_get_qp_type(struct rdma_ucm_create_id *cmd, enum ib_qp_type *qp_type)\n{\n	switch (cmd->ps) {\n	case RDMA_PS_TCP:\n		*qp_type = IB_QPT_RC;\n		return 0;\n	case RDMA_PS_UDP:\n	case RDMA_PS_IPOIB:\n		*qp_type = IB_QPT_UD;\n		return 0;\n	case RDMA_PS_IB:\n		*qp_type = cmd->qp_type;\n		return 0;\n	default:\n		return -EINVAL;\n	}\n}\n\nstatic ssize_t ucma_create_id(struct ucma_file *file, const char __user *inbuf,\n			      int in_len, int out_len)\n{\n	struct rdma_ucm_create_id cmd;\n	struct rdma_ucm_create_id_resp resp;\n	struct ucma_context *ctx;\n	enum ib_qp_type qp_type;\n	int ret;\n\n	if (out_len < sizeof(resp))\n		return -ENOSPC;\n\n	if (copy_from_user(&cmd, inbuf, sizeof(cmd)))\n		return -EFAULT;\n\n	ret = ucma_get_qp_type(&cmd, &qp_type);\n	if (ret)\n		return ret;\n\n	mutex_lock(&file->mut);\n	ctx = ucma_alloc_ctx(file);\n	mutex_unlock(&file->mut);\n	if (!ctx)\n		return -ENOMEM;\n\n	ctx->uid = cmd.uid;\n	ctx->cm_id = rdma_create_id(ucma_event_handler, ctx, cmd.ps, qp_type);\n	if (IS_ERR(ctx->cm_id)) {\n		ret = PTR_ERR(ctx->cm_id);\n		goto err1;\n	}\n\n	resp.id = ctx->id;\n	if (copy_to_user((void __user *)(unsigned long)cmd.response,\n			 &resp, sizeof(resp))) {\n		ret = -EFAULT;\n		goto err2;\n	}\n	return 0;\n\nerr2:\n	rdma_destroy_id(ctx->cm_id);\nerr1:\n	mutex_lock(&mut);\n	idr_remove(&ctx_idr, ctx->id);\n	mutex_unlock(&mut);\n	kfree(ctx);\n	return ret;\n}\n\nstatic void ucma_cleanup_multicast(struct ucma_context *ctx)\n{\n	struct ucma_multicast *mc, *tmp;\n\n	mutex_lock(&mut);\n	list_for_each_entry_safe(mc, tmp, &ctx->mc_list, list) {\n		list_del(&mc->list);\n		idr_remove(&multicast_idr, mc->id);\n		kfree(mc);\n	}\n	mutex_unlock(&mut);\n}\n\nstatic void ucma_cleanup_mc_events(struct ucma_multicast *mc)\n{\n	struct ucma_event *uevent, *tmp;\n\n	list_for_each_entry_safe(uevent, tmp, &mc->ctx->file->event_list, list) {\n		if (uevent->mc != mc)\n			continue;\n\n		list_del(&uevent->list);\n		kfree(uevent);\n	}\n}\n\n/*\n * We cannot hold file->mut when calling rdma_destroy_id() or we can\n * deadlock.  We also acquire file->mut in ucma_event_handler(), and\n * rdma_destroy_id() will wait until all callbacks have completed.\n */\nstatic int ucma_free_ctx(struct ucma_context *ctx)\n{\n	int events_reported;\n	struct ucma_event *uevent, *tmp;\n	LIST_HEAD(list);\n\n	/* No new events will be generated after destroying the id. */\n	rdma_destroy_id(ctx->cm_id);\n\n	ucma_cleanup_multicast(ctx);\n\n	/* Cleanup events not yet reported to the user. */\n	mutex_lock(&ctx->file->mut);\n	list_for_each_entry_safe(uevent, tmp, &ctx->file->event_list, list) {\n		if (uevent->ctx == ctx)\n			list_move_tail(&uevent->list, &list);\n	}\n	list_del(&ctx->list);\n	mutex_unlock(&ctx->file->mut);\n\n	list_for_each_entry_safe(uevent, tmp, &list, list) {\n		list_del(&uevent->list);\n		if (uevent->resp.event == RDMA_CM_EVENT_CONNECT_REQUEST)\n			rdma_destroy_id(uevent->cm_id);\n		kfree(uevent);\n	}\n\n	events_reported = ctx->events_reported;\n	kfree(ctx);\n	return events_reported;\n}\n\nstatic ssize_t ucma_destroy_id(struct ucma_file *file, const char __user *inbuf,\n			       int in_len, int out_len)\n{\n	struct rdma_ucm_destroy_id cmd;\n	struct rdma_ucm_destroy_id_resp resp;\n	struct ucma_context *ctx;\n	int ret = 0;\n\n	if (out_len < sizeof(resp))\n		return -ENOSPC;\n\n	if (copy_from_user(&cmd, inbuf, sizeof(cmd)))\n		return -EFAULT;\n\n	mutex_lock(&mut);\n	ctx = _ucma_find_context(cmd.id, file);\n	if (!IS_ERR(ctx))\n		idr_remove(&ctx_idr, ctx->id);\n	mutex_unlock(&mut);\n\n	if (IS_ERR(ctx))\n		return PTR_ERR(ctx);\n\n	ucma_put_ctx(ctx);\n	wait_for_completion(&ctx->comp);\n	resp.events_reported = ucma_free_ctx(ctx);\n\n	if (copy_to_user((void __user *)(unsigned long)cmd.response,\n			 &resp, sizeof(resp)))\n		ret = -EFAULT;\n\n	return ret;\n}\n\nstatic ssize_t ucma_bind_ip(struct ucma_file *file, const char __user *inbuf,\n			      int in_len, int out_len)\n{\n	struct rdma_ucm_bind_ip cmd;\n	struct ucma_context *ctx;\n	int ret;\n\n	if (copy_from_user(&cmd, inbuf, sizeof(cmd)))\n		return -EFAULT;\n\n	ctx = ucma_get_ctx(file, cmd.id);\n	if (IS_ERR(ctx))\n		return PTR_ERR(ctx);\n\n	ret = rdma_bind_addr(ctx->cm_id, (struct sockaddr *) &cmd.addr);\n	ucma_put_ctx(ctx);\n	return ret;\n}\n\nstatic ssize_t ucma_bind(struct ucma_file *file, const char __user *inbuf,\n			 int in_len, int out_len)\n{\n	struct rdma_ucm_bind cmd;\n	struct sockaddr *addr;\n	struct ucma_context *ctx;\n	int ret;\n\n	if (copy_from_user(&cmd, inbuf, sizeof(cmd)))\n		return -EFAULT;\n\n	addr = (struct sockaddr *) &cmd.addr;\n	if (cmd.reserved || !cmd.addr_size || (cmd.addr_size != rdma_addr_size(addr)))\n		return -EINVAL;\n\n	ctx = ucma_get_ctx(file, cmd.id);\n	if (IS_ERR(ctx))\n		return PTR_ERR(ctx);\n\n	ret = rdma_bind_addr(ctx->cm_id, addr);\n	ucma_put_ctx(ctx);\n	return ret;\n}\n\nstatic ssize_t ucma_resolve_ip(struct ucma_file *file,\n			       const char __user *inbuf,\n			       int in_len, int out_len)\n{\n	struct rdma_ucm_resolve_ip cmd;\n	struct ucma_context *ctx;\n	int ret;\n\n	if (copy_from_user(&cmd, inbuf, sizeof(cmd)))\n		return -EFAULT;\n\n	ctx = ucma_get_ctx(file, cmd.id);\n	if (IS_ERR(ctx))\n		return PTR_ERR(ctx);\n\n	ret = rdma_resolve_addr(ctx->cm_id, (struct sockaddr *) &cmd.src_addr,\n				(struct sockaddr *) &cmd.dst_addr,\n				cmd.timeout_ms);\n	ucma_put_ctx(ctx);\n	return ret;\n}\n\nstatic ssize_t ucma_resolve_addr(struct ucma_file *file,\n				 const char __user *inbuf,\n				 int in_len, int out_len)\n{\n	struct rdma_ucm_resolve_addr cmd;\n	struct sockaddr *src, *dst;\n	struct ucma_context *ctx;\n	int ret;\n\n	if (copy_from_user(&cmd, inbuf, sizeof(cmd)))\n		return -EFAULT;\n\n	src = (struct sockaddr *) &cmd.src_addr;\n	dst = (struct sockaddr *) &cmd.dst_addr;\n	if (cmd.reserved || (cmd.src_size && (cmd.src_size != rdma_addr_size(src))) ||\n	    !cmd.dst_size || (cmd.dst_size != rdma_addr_size(dst)))\n		return -EINVAL;\n\n	ctx = ucma_get_ctx(file, cmd.id);\n	if (IS_ERR(ctx))\n		return PTR_ERR(ctx);\n\n	ret = rdma_resolve_addr(ctx->cm_id, src, dst, cmd.timeout_ms);\n	ucma_put_ctx(ctx);\n	return ret;\n}\n\nstatic ssize_t ucma_resolve_route(struct ucma_file *file,\n				  const char __user *inbuf,\n				  int in_len, int out_len)\n{\n	struct rdma_ucm_resolve_route cmd;\n	struct ucma_context *ctx;\n	int ret;\n\n	if (copy_from_user(&cmd, inbuf, sizeof(cmd)))\n		return -EFAULT;\n\n	ctx = ucma_get_ctx(file, cmd.id);\n	if (IS_ERR(ctx))\n		return PTR_ERR(ctx);\n\n	ret = rdma_resolve_route(ctx->cm_id, cmd.timeout_ms);\n	ucma_put_ctx(ctx);\n	return ret;\n}\n\nstatic void ucma_copy_ib_route(struct rdma_ucm_query_route_resp *resp,\n			       struct rdma_route *route)\n{\n	struct rdma_dev_addr *dev_addr;\n\n	resp->num_paths = route->num_paths;\n	switch (route->num_paths) {\n	case 0:\n		dev_addr = &route->addr.dev_addr;\n		rdma_addr_get_dgid(dev_addr,\n				   (union ib_gid *) &resp->ib_route[0].dgid);\n		rdma_addr_get_sgid(dev_addr,\n				   (union ib_gid *) &resp->ib_route[0].sgid);\n		resp->ib_route[0].pkey = cpu_to_be16(ib_addr_get_pkey(dev_addr));\n		break;\n	case 2:\n		ib_copy_path_rec_to_user(&resp->ib_route[1],\n					 &route->path_rec[1]);\n		/* fall through */\n	case 1:\n		ib_copy_path_rec_to_user(&resp->ib_route[0],\n					 &route->path_rec[0]);\n		break;\n	default:\n		break;\n	}\n}\n\nstatic void ucma_copy_iboe_route(struct rdma_ucm_query_route_resp *resp,\n				 struct rdma_route *route)\n{\n\n	resp->num_paths = route->num_paths;\n	switch (route->num_paths) {\n	case 0:\n		rdma_ip2gid((struct sockaddr *)&route->addr.dst_addr,\n			    (union ib_gid *)&resp->ib_route[0].dgid);\n		rdma_ip2gid((struct sockaddr *)&route->addr.src_addr,\n			    (union ib_gid *)&resp->ib_route[0].sgid);\n		resp->ib_route[0].pkey = cpu_to_be16(0xffff);\n		break;\n	case 2:\n		ib_copy_path_rec_to_user(&resp->ib_route[1],\n					 &route->path_rec[1]);\n		/* fall through */\n	case 1:\n		ib_copy_path_rec_to_user(&resp->ib_route[0],\n					 &route->path_rec[0]);\n		break;\n	default:\n		break;\n	}\n}\n\nstatic void ucma_copy_iw_route(struct rdma_ucm_query_route_resp *resp,\n			       struct rdma_route *route)\n{\n	struct rdma_dev_addr *dev_addr;\n\n	dev_addr = &route->addr.dev_addr;\n	rdma_addr_get_dgid(dev_addr, (union ib_gid *) &resp->ib_route[0].dgid);\n	rdma_addr_get_sgid(dev_addr, (union ib_gid *) &resp->ib_route[0].sgid);\n}\n\nstatic ssize_t ucma_query_route(struct ucma_file *file,\n				const char __user *inbuf,\n				int in_len, int out_len)\n{\n	struct rdma_ucm_query cmd;\n	struct rdma_ucm_query_route_resp resp;\n	struct ucma_context *ctx;\n	struct sockaddr *addr;\n	int ret = 0;\n\n	if (out_len < sizeof(resp))\n		return -ENOSPC;\n\n	if (copy_from_user(&cmd, inbuf, sizeof(cmd)))\n		return -EFAULT;\n\n	ctx = ucma_get_ctx(file, cmd.id);\n	if (IS_ERR(ctx))\n		return PTR_ERR(ctx);\n\n	memset(&resp, 0, sizeof resp);\n	addr = (struct sockaddr *) &ctx->cm_id->route.addr.src_addr;\n	memcpy(&resp.src_addr, addr, addr->sa_family == AF_INET ?\n				     sizeof(struct sockaddr_in) :\n				     sizeof(struct sockaddr_in6));\n	addr = (struct sockaddr *) &ctx->cm_id->route.addr.dst_addr;\n	memcpy(&resp.dst_addr, addr, addr->sa_family == AF_INET ?\n				     sizeof(struct sockaddr_in) :\n				     sizeof(struct sockaddr_in6));\n	if (!ctx->cm_id->device)\n		goto out;\n\n	resp.node_guid = (__force __u64) ctx->cm_id->device->node_guid;\n	resp.port_num = ctx->cm_id->port_num;\n	switch (rdma_node_get_transport(ctx->cm_id->device->node_type)) {\n	case RDMA_TRANSPORT_IB:\n		switch (rdma_port_get_link_layer(ctx->cm_id->device,\n			ctx->cm_id->port_num)) {\n		case IB_LINK_LAYER_INFINIBAND:\n			ucma_copy_ib_route(&resp, &ctx->cm_id->route);\n			break;\n		case IB_LINK_LAYER_ETHERNET:\n			ucma_copy_iboe_route(&resp, &ctx->cm_id->route);\n			break;\n		default:\n			break;\n		}\n		break;\n	case RDMA_TRANSPORT_IWARP:\n		ucma_copy_iw_route(&resp, &ctx->cm_id->route);\n		break;\n	default:\n		break;\n	}\n\nout:\n	if (copy_to_user((void __user *)(unsigned long)cmd.response,\n			 &resp, sizeof(resp)))\n		ret = -EFAULT;\n\n	ucma_put_ctx(ctx);\n	return ret;\n}\n\nstatic void ucma_query_device_addr(struct rdma_cm_id *cm_id,\n				   struct rdma_ucm_query_addr_resp *resp)\n{\n	if (!cm_id->device)\n		return;\n\n	resp->node_guid = (__force __u64) cm_id->device->node_guid;\n	resp->port_num = cm_id->port_num;\n	resp->pkey = (__force __u16) cpu_to_be16(\n		     ib_addr_get_pkey(&cm_id->route.addr.dev_addr));\n}\n\nstatic ssize_t ucma_query_addr(struct ucma_context *ctx,\n			       void __user *response, int out_len)\n{\n	struct rdma_ucm_query_addr_resp resp;\n	struct sockaddr *addr;\n	int ret = 0;\n\n	if (out_len < sizeof(resp))\n		return -ENOSPC;\n\n	memset(&resp, 0, sizeof resp);\n\n	addr = (struct sockaddr *) &ctx->cm_id->route.addr.src_addr;\n	resp.src_size = rdma_addr_size(addr);\n	memcpy(&resp.src_addr, addr, resp.src_size);\n\n	addr = (struct sockaddr *) &ctx->cm_id->route.addr.dst_addr;\n	resp.dst_size = rdma_addr_size(addr);\n	memcpy(&resp.dst_addr, addr, resp.dst_size);\n\n	ucma_query_device_addr(ctx->cm_id, &resp);\n\n	if (copy_to_user(response, &resp, sizeof(resp)))\n		ret = -EFAULT;\n\n	return ret;\n}\n\nstatic ssize_t ucma_query_path(struct ucma_context *ctx,\n			       void __user *response, int out_len)\n{\n	struct rdma_ucm_query_path_resp *resp;\n	int i, ret = 0;\n\n	if (out_len < sizeof(*resp))\n		return -ENOSPC;\n\n	resp = kzalloc(out_len, GFP_KERNEL);\n	if (!resp)\n		return -ENOMEM;\n\n	resp->num_paths = ctx->cm_id->route.num_paths;\n	for (i = 0, out_len -= sizeof(*resp);\n	     i < resp->num_paths && out_len > sizeof(struct ib_path_rec_data);\n	     i++, out_len -= sizeof(struct ib_path_rec_data)) {\n\n		resp->path_data[i].flags = IB_PATH_GMP | IB_PATH_PRIMARY |\n					   IB_PATH_BIDIRECTIONAL;\n		ib_sa_pack_path(&ctx->cm_id->route.path_rec[i],\n				&resp->path_data[i].path_rec);\n	}\n\n	if (copy_to_user(response, resp,\n			 sizeof(*resp) + (i * sizeof(struct ib_path_rec_data))))\n		ret = -EFAULT;\n\n	kfree(resp);\n	return ret;\n}\n\nstatic ssize_t ucma_query_gid(struct ucma_context *ctx,\n			      void __user *response, int out_len)\n{\n	struct rdma_ucm_query_addr_resp resp;\n	struct sockaddr_ib *addr;\n	int ret = 0;\n\n	if (out_len < sizeof(resp))\n		return -ENOSPC;\n\n	memset(&resp, 0, sizeof resp);\n\n	ucma_query_device_addr(ctx->cm_id, &resp);\n\n	addr = (struct sockaddr_ib *) &resp.src_addr;\n	resp.src_size = sizeof(*addr);\n	if (ctx->cm_id->route.addr.src_addr.ss_family == AF_IB) {\n		memcpy(addr, &ctx->cm_id->route.addr.src_addr, resp.src_size);\n	} else {\n		addr->sib_family = AF_IB;\n		addr->sib_pkey = (__force __be16) resp.pkey;\n		rdma_addr_get_sgid(&ctx->cm_id->route.addr.dev_addr,\n				   (union ib_gid *) &addr->sib_addr);\n		addr->sib_sid = rdma_get_service_id(ctx->cm_id, (struct sockaddr *)\n						    &ctx->cm_id->route.addr.src_addr);\n	}\n\n	addr = (struct sockaddr_ib *) &resp.dst_addr;\n	resp.dst_size = sizeof(*addr);\n	if (ctx->cm_id->route.addr.dst_addr.ss_family == AF_IB) {\n		memcpy(addr, &ctx->cm_id->route.addr.dst_addr, resp.dst_size);\n	} else {\n		addr->sib_family = AF_IB;\n		addr->sib_pkey = (__force __be16) resp.pkey;\n		rdma_addr_get_dgid(&ctx->cm_id->route.addr.dev_addr,\n				   (union ib_gid *) &addr->sib_addr);\n		addr->sib_sid = rdma_get_service_id(ctx->cm_id, (struct sockaddr *)\n						    &ctx->cm_id->route.addr.dst_addr);\n	}\n\n	if (copy_to_user(response, &resp, sizeof(resp)))\n		ret = -EFAULT;\n\n	return ret;\n}\n\nstatic ssize_t ucma_query(struct ucma_file *file,\n			  const char __user *inbuf,\n			  int in_len, int out_len)\n{\n	struct rdma_ucm_query cmd;\n	struct ucma_context *ctx;\n	void __user *response;\n	int ret;\n\n	if (copy_from_user(&cmd, inbuf, sizeof(cmd)))\n		return -EFAULT;\n\n	response = (void __user *)(unsigned long) cmd.response;\n	ctx = ucma_get_ctx(file, cmd.id);\n	if (IS_ERR(ctx))\n		return PTR_ERR(ctx);\n\n	switch (cmd.option) {\n	case RDMA_USER_CM_QUERY_ADDR:\n		ret = ucma_query_addr(ctx, response, out_len);\n		break;\n	case RDMA_USER_CM_QUERY_PATH:\n		ret = ucma_query_path(ctx, response, out_len);\n		break;\n	case RDMA_USER_CM_QUERY_GID:\n		ret = ucma_query_gid(ctx, response, out_len);\n		break;\n	default:\n		ret = -ENOSYS;\n		break;\n	}\n\n	ucma_put_ctx(ctx);\n	return ret;\n}\n\nstatic void ucma_copy_conn_param(struct rdma_cm_id *id,\n				 struct rdma_conn_param *dst,\n				 struct rdma_ucm_conn_param *src)\n{\n	dst->private_data = src->private_data;\n	dst->private_data_len = src->private_data_len;\n	dst->responder_resources =src->responder_resources;\n	dst->initiator_depth = src->initiator_depth;\n	dst->flow_control = src->flow_control;\n	dst->retry_count = src->retry_count;\n	dst->rnr_retry_count = src->rnr_retry_count;\n	dst->srq = src->srq;\n	dst->qp_num = src->qp_num;\n	dst->qkey = (id->route.addr.src_addr.ss_family == AF_IB) ? src->qkey : 0;\n}\n\nstatic ssize_t ucma_connect(struct ucma_file *file, const char __user *inbuf,\n			    int in_len, int out_len)\n{\n	struct rdma_ucm_connect cmd;\n	struct rdma_conn_param conn_param;\n	struct ucma_context *ctx;\n	int ret;\n\n	if (copy_from_user(&cmd, inbuf, sizeof(cmd)))\n		return -EFAULT;\n\n	if (!cmd.conn_param.valid)\n		return -EINVAL;\n\n	ctx = ucma_get_ctx(file, cmd.id);\n	if (IS_ERR(ctx))\n		return PTR_ERR(ctx);\n\n	ucma_copy_conn_param(ctx->cm_id, &conn_param, &cmd.conn_param);\n	ret = rdma_connect(ctx->cm_id, &conn_param);\n	ucma_put_ctx(ctx);\n	return ret;\n}\n\nstatic ssize_t ucma_listen(struct ucma_file *file, const char __user *inbuf,\n			   int in_len, int out_len)\n{\n	struct rdma_ucm_listen cmd;\n	struct ucma_context *ctx;\n	int ret;\n\n	if (copy_from_user(&cmd, inbuf, sizeof(cmd)))\n		return -EFAULT;\n\n	ctx = ucma_get_ctx(file, cmd.id);\n	if (IS_ERR(ctx))\n		return PTR_ERR(ctx);\n\n	ctx->backlog = cmd.backlog > 0 && cmd.backlog < max_backlog ?\n		       cmd.backlog : max_backlog;\n	ret = rdma_listen(ctx->cm_id, ctx->backlog);\n	ucma_put_ctx(ctx);\n	return ret;\n}\n\nstatic ssize_t ucma_accept(struct ucma_file *file, const char __user *inbuf,\n			   int in_len, int out_len)\n{\n	struct rdma_ucm_accept cmd;\n	struct rdma_conn_param conn_param;\n	struct ucma_context *ctx;\n	int ret;\n\n	if (copy_from_user(&cmd, inbuf, sizeof(cmd)))\n		return -EFAULT;\n\n	ctx = ucma_get_ctx(file, cmd.id);\n	if (IS_ERR(ctx))\n		return PTR_ERR(ctx);\n\n	if (cmd.conn_param.valid) {\n		ucma_copy_conn_param(ctx->cm_id, &conn_param, &cmd.conn_param);\n		mutex_lock(&file->mut);\n		ret = rdma_accept(ctx->cm_id, &conn_param);\n		if (!ret)\n			ctx->uid = cmd.uid;\n		mutex_unlock(&file->mut);\n	} else\n		ret = rdma_accept(ctx->cm_id, NULL);\n\n	ucma_put_ctx(ctx);\n	return ret;\n}\n\nstatic ssize_t ucma_reject(struct ucma_file *file, const char __user *inbuf,\n			   int in_len, int out_len)\n{\n	struct rdma_ucm_reject cmd;\n	struct ucma_context *ctx;\n	int ret;\n\n	if (copy_from_user(&cmd, inbuf, sizeof(cmd)))\n		return -EFAULT;\n\n	ctx = ucma_get_ctx(file, cmd.id);\n	if (IS_ERR(ctx))\n		return PTR_ERR(ctx);\n\n	ret = rdma_reject(ctx->cm_id, cmd.private_data, cmd.private_data_len);\n	ucma_put_ctx(ctx);\n	return ret;\n}\n\nstatic ssize_t ucma_disconnect(struct ucma_file *file, const char __user *inbuf,\n			       int in_len, int out_len)\n{\n	struct rdma_ucm_disconnect cmd;\n	struct ucma_context *ctx;\n	int ret;\n\n	if (copy_from_user(&cmd, inbuf, sizeof(cmd)))\n		return -EFAULT;\n\n	ctx = ucma_get_ctx(file, cmd.id);\n	if (IS_ERR(ctx))\n		return PTR_ERR(ctx);\n\n	ret = rdma_disconnect(ctx->cm_id);\n	ucma_put_ctx(ctx);\n	return ret;\n}\n\nstatic ssize_t ucma_init_qp_attr(struct ucma_file *file,\n				 const char __user *inbuf,\n				 int in_len, int out_len)\n{\n	struct rdma_ucm_init_qp_attr cmd;\n	struct ib_uverbs_qp_attr resp;\n	struct ucma_context *ctx;\n	struct ib_qp_attr qp_attr;\n	int ret;\n\n	if (out_len < sizeof(resp))\n		return -ENOSPC;\n\n	if (copy_from_user(&cmd, inbuf, sizeof(cmd)))\n		return -EFAULT;\n\n	ctx = ucma_get_ctx(file, cmd.id);\n	if (IS_ERR(ctx))\n		return PTR_ERR(ctx);\n\n	resp.qp_attr_mask = 0;\n	memset(&qp_attr, 0, sizeof qp_attr);\n	qp_attr.qp_state = cmd.qp_state;\n	ret = rdma_init_qp_attr(ctx->cm_id, &qp_attr, &resp.qp_attr_mask);\n	if (ret)\n		goto out;\n\n	ib_copy_qp_attr_to_user(&resp, &qp_attr);\n	if (copy_to_user((void __user *)(unsigned long)cmd.response,\n			 &resp, sizeof(resp)))\n		ret = -EFAULT;\n\nout:\n	ucma_put_ctx(ctx);\n	return ret;\n}\n\nstatic int ucma_set_option_id(struct ucma_context *ctx, int optname,\n			      void *optval, size_t optlen)\n{\n	int ret = 0;\n\n	switch (optname) {\n	case RDMA_OPTION_ID_TOS:\n		if (optlen != sizeof(u8)) {\n			ret = -EINVAL;\n			break;\n		}\n		rdma_set_service_type(ctx->cm_id, *((u8 *) optval));\n		break;\n	case RDMA_OPTION_ID_REUSEADDR:\n		if (optlen != sizeof(int)) {\n			ret = -EINVAL;\n			break;\n		}\n		ret = rdma_set_reuseaddr(ctx->cm_id, *((int *) optval) ? 1 : 0);\n		break;\n	case RDMA_OPTION_ID_AFONLY:\n		if (optlen != sizeof(int)) {\n			ret = -EINVAL;\n			break;\n		}\n		ret = rdma_set_afonly(ctx->cm_id, *((int *) optval) ? 1 : 0);\n		break;\n	default:\n		ret = -ENOSYS;\n	}\n\n	return ret;\n}\n\nstatic int ucma_set_ib_path(struct ucma_context *ctx,\n			    struct ib_path_rec_data *path_data, size_t optlen)\n{\n	struct ib_sa_path_rec sa_path;\n	struct rdma_cm_event event;\n	int ret;\n\n	if (optlen % sizeof(*path_data))\n		return -EINVAL;\n\n	for (; optlen; optlen -= sizeof(*path_data), path_data++) {\n		if (path_data->flags == (IB_PATH_GMP | IB_PATH_PRIMARY |\n					 IB_PATH_BIDIRECTIONAL))\n			break;\n	}\n\n	if (!optlen)\n		return -EINVAL;\n\n	ib_sa_unpack_path(path_data->path_rec, &sa_path);\n	ret = rdma_set_ib_paths(ctx->cm_id, &sa_path, 1);\n	if (ret)\n		return ret;\n\n	memset(&event, 0, sizeof event);\n	event.event = RDMA_CM_EVENT_ROUTE_RESOLVED;\n	return ucma_event_handler(ctx->cm_id, &event);\n}\n\nstatic int ucma_set_option_ib(struct ucma_context *ctx, int optname,\n			      void *optval, size_t optlen)\n{\n	int ret;\n\n	switch (optname) {\n	case RDMA_OPTION_IB_PATH:\n		ret = ucma_set_ib_path(ctx, optval, optlen);\n		break;\n	default:\n		ret = -ENOSYS;\n	}\n\n	return ret;\n}\n\nstatic int ucma_set_option_level(struct ucma_context *ctx, int level,\n				 int optname, void *optval, size_t optlen)\n{\n	int ret;\n\n	switch (level) {\n	case RDMA_OPTION_ID:\n		ret = ucma_set_option_id(ctx, optname, optval, optlen);\n		break;\n	case RDMA_OPTION_IB:\n		ret = ucma_set_option_ib(ctx, optname, optval, optlen);\n		break;\n	default:\n		ret = -ENOSYS;\n	}\n\n	return ret;\n}\n\nstatic ssize_t ucma_set_option(struct ucma_file *file, const char __user *inbuf,\n			       int in_len, int out_len)\n{\n	struct rdma_ucm_set_option cmd;\n	struct ucma_context *ctx;\n	void *optval;\n	int ret;\n\n	if (copy_from_user(&cmd, inbuf, sizeof(cmd)))\n		return -EFAULT;\n\n	ctx = ucma_get_ctx(file, cmd.id);\n	if (IS_ERR(ctx))\n		return PTR_ERR(ctx);\n\n	optval = memdup_user((void __user *) (unsigned long) cmd.optval,\n			     cmd.optlen);\n	if (IS_ERR(optval)) {\n		ret = PTR_ERR(optval);\n		goto out;\n	}\n\n	ret = ucma_set_option_level(ctx, cmd.level, cmd.optname, optval,\n				    cmd.optlen);\n	kfree(optval);\n\nout:\n	ucma_put_ctx(ctx);\n	return ret;\n}\n\nstatic ssize_t ucma_notify(struct ucma_file *file, const char __user *inbuf,\n			   int in_len, int out_len)\n{\n	struct rdma_ucm_notify cmd;\n	struct ucma_context *ctx;\n	int ret;\n\n	if (copy_from_user(&cmd, inbuf, sizeof(cmd)))\n		return -EFAULT;\n\n	ctx = ucma_get_ctx(file, cmd.id);\n	if (IS_ERR(ctx))\n		return PTR_ERR(ctx);\n\n	ret = rdma_notify(ctx->cm_id, (enum ib_event_type) cmd.event);\n	ucma_put_ctx(ctx);\n	return ret;\n}\n\nstatic ssize_t ucma_process_join(struct ucma_file *file,\n				 struct rdma_ucm_join_mcast *cmd,  int out_len)\n{\n	struct rdma_ucm_create_id_resp resp;\n	struct ucma_context *ctx;\n	struct ucma_multicast *mc;\n	struct sockaddr *addr;\n	int ret;\n\n	if (out_len < sizeof(resp))\n		return -ENOSPC;\n\n	addr = (struct sockaddr *) &cmd->addr;\n	if (cmd->reserved || !cmd->addr_size || (cmd->addr_size != rdma_addr_size(addr)))\n		return -EINVAL;\n\n	ctx = ucma_get_ctx(file, cmd->id);\n	if (IS_ERR(ctx))\n		return PTR_ERR(ctx);\n\n	mutex_lock(&file->mut);\n	mc = ucma_alloc_multicast(ctx);\n	if (!mc) {\n		ret = -ENOMEM;\n		goto err1;\n	}\n\n	mc->uid = cmd->uid;\n	memcpy(&mc->addr, addr, cmd->addr_size);\n	ret = rdma_join_multicast(ctx->cm_id, (struct sockaddr *) &mc->addr, mc);\n	if (ret)\n		goto err2;\n\n	resp.id = mc->id;\n	if (copy_to_user((void __user *)(unsigned long) cmd->response,\n			 &resp, sizeof(resp))) {\n		ret = -EFAULT;\n		goto err3;\n	}\n\n	mutex_unlock(&file->mut);\n	ucma_put_ctx(ctx);\n	return 0;\n\nerr3:\n	rdma_leave_multicast(ctx->cm_id, (struct sockaddr *) &mc->addr);\n	ucma_cleanup_mc_events(mc);\nerr2:\n	mutex_lock(&mut);\n	idr_remove(&multicast_idr, mc->id);\n	mutex_unlock(&mut);\n	list_del(&mc->list);\n	kfree(mc);\nerr1:\n	mutex_unlock(&file->mut);\n	ucma_put_ctx(ctx);\n	return ret;\n}\n\nstatic ssize_t ucma_join_ip_multicast(struct ucma_file *file,\n				      const char __user *inbuf,\n				      int in_len, int out_len)\n{\n	struct rdma_ucm_join_ip_mcast cmd;\n	struct rdma_ucm_join_mcast join_cmd;\n\n	if (copy_from_user(&cmd, inbuf, sizeof(cmd)))\n		return -EFAULT;\n\n	join_cmd.response = cmd.response;\n	join_cmd.uid = cmd.uid;\n	join_cmd.id = cmd.id;\n	join_cmd.addr_size = rdma_addr_size((struct sockaddr *) &cmd.addr);\n	join_cmd.reserved = 0;\n	memcpy(&join_cmd.addr, &cmd.addr, join_cmd.addr_size);\n\n	return ucma_process_join(file, &join_cmd, out_len);\n}\n\nstatic ssize_t ucma_join_multicast(struct ucma_file *file,\n				   const char __user *inbuf,\n				   int in_len, int out_len)\n{\n	struct rdma_ucm_join_mcast cmd;\n\n	if (copy_from_user(&cmd, inbuf, sizeof(cmd)))\n		return -EFAULT;\n\n	return ucma_process_join(file, &cmd, out_len);\n}\n\nstatic ssize_t ucma_leave_multicast(struct ucma_file *file,\n				    const char __user *inbuf,\n				    int in_len, int out_len)\n{\n	struct rdma_ucm_destroy_id cmd;\n	struct rdma_ucm_destroy_id_resp resp;\n	struct ucma_multicast *mc;\n	int ret = 0;\n\n	if (out_len < sizeof(resp))\n		return -ENOSPC;\n\n	if (copy_from_user(&cmd, inbuf, sizeof(cmd)))\n		return -EFAULT;\n\n	mutex_lock(&mut);\n	mc = idr_find(&multicast_idr, cmd.id);\n	if (!mc)\n		mc = ERR_PTR(-ENOENT);\n	else if (mc->ctx->file != file)\n		mc = ERR_PTR(-EINVAL);\n	else {\n		idr_remove(&multicast_idr, mc->id);\n		atomic_inc(&mc->ctx->ref);\n	}\n	mutex_unlock(&mut);\n\n	if (IS_ERR(mc)) {\n		ret = PTR_ERR(mc);\n		goto out;\n	}\n\n	rdma_leave_multicast(mc->ctx->cm_id, (struct sockaddr *) &mc->addr);\n	mutex_lock(&mc->ctx->file->mut);\n	ucma_cleanup_mc_events(mc);\n	list_del(&mc->list);\n	mutex_unlock(&mc->ctx->file->mut);\n\n	ucma_put_ctx(mc->ctx);\n	resp.events_reported = mc->events_reported;\n	kfree(mc);\n\n	if (copy_to_user((void __user *)(unsigned long)cmd.response,\n			 &resp, sizeof(resp)))\n		ret = -EFAULT;\nout:\n	return ret;\n}\n\nstatic void ucma_lock_files(struct ucma_file *file1, struct ucma_file *file2)\n{\n	/* Acquire mutex\'s based on pointer comparison to prevent deadlock. */\n	if (file1 < file2) {\n		mutex_lock(&file1->mut);\n		mutex_lock(&file2->mut);\n	} else {\n		mutex_lock(&file2->mut);\n		mutex_lock(&file1->mut);\n	}\n}\n\nstatic void ucma_unlock_files(struct ucma_file *file1, struct ucma_file *file2)\n{\n	if (file1 < file2) {\n		mutex_unlock(&file2->mut);\n		mutex_unlock(&file1->mut);\n	} else {\n		mutex_unlock(&file1->mut);\n		mutex_unlock(&file2->mut);\n	}\n}\n\nstatic void ucma_move_events(struct ucma_context *ctx, struct ucma_file *file)\n{\n	struct ucma_event *uevent, *tmp;\n\n	list_for_each_entry_safe(uevent, tmp, &ctx->file->event_list, list)\n		if (uevent->ctx == ctx)\n			list_move_tail(&uevent->list, &file->event_list);\n}\n\nstatic ssize_t ucma_migrate_id(struct ucma_file *new_file,\n			       const char __user *inbuf,\n			       int in_len, int out_len)\n{\n	struct rdma_ucm_migrate_id cmd;\n	struct rdma_ucm_migrate_resp resp;\n	struct ucma_context *ctx;\n	struct fd f;\n	struct ucma_file *cur_file;\n	int ret = 0;\n\n	if (copy_from_user(&cmd, inbuf, sizeof(cmd)))\n		return -EFAULT;\n\n	/* Get current fd to protect against it being closed */\n	f = fdget(cmd.fd);\n	if (!f.file)\n		return -ENOENT;\n\n	/* Validate current fd and prevent destruction of id. */\n	ctx = ucma_get_ctx(f.file->private_data, cmd.id);\n	if (IS_ERR(ctx)) {\n		ret = PTR_ERR(ctx);\n		goto file_put;\n	}\n\n	cur_file = ctx->file;\n	if (cur_file == new_file) {\n		resp.events_reported = ctx->events_reported;\n		goto response;\n	}\n\n	/*\n	 * Migrate events between fd\'s, maintaining order, and avoiding new\n	 * events being added before existing events.\n	 */\n	ucma_lock_files(cur_file, new_file);\n	mutex_lock(&mut);\n\n	list_move_tail(&ctx->list, &new_file->ctx_list);\n	ucma_move_events(ctx, new_file);\n	ctx->file = new_file;\n	resp.events_reported = ctx->events_reported;\n\n	mutex_unlock(&mut);\n	ucma_unlock_files(cur_file, new_file);\n\nresponse:\n	if (copy_to_user((void __user *)(unsigned long)cmd.response,\n			 &resp, sizeof(resp)))\n		ret = -EFAULT;\n\n	ucma_put_ctx(ctx);\nfile_put:\n	fdput(f);\n	return ret;\n}\n\nstatic inline int kvaser_usb_send_msg(const struct kvaser_usb *dev,\n				      struct kvaser_msg *msg)\n{\n	int actual_len;\n\n	return usb_bulk_msg(dev->udev,\n			    usb_sndbulkpipe(dev->udev,\n					dev->bulk_out->bEndpointAddress),\n			    msg, msg->len, &actual_len,\n			    USB_SEND_TIMEOUT);\n}\n\nstatic int kvaser_usb_send_simple_msg(const struct kvaser_usb *dev,\n				      u8 msg_id, int channel)\n{\n	struct kvaser_msg *msg;\n	int rc;\n\n	msg = kmalloc(sizeof(*msg), GFP_KERNEL);\n	if (!msg)\n		return -ENOMEM;\n\n	msg->id = msg_id;\n	msg->len = MSG_HEADER_LEN + sizeof(struct kvaser_msg_simple);\n	msg->u.simple.channel = channel;\n	msg->u.simple.tid = 0xff;\n\n	rc = kvaser_usb_send_msg(dev, msg);\n\n	kfree(msg);\n	return rc;\n}\n\nstatic int kvaser_usb_get_software_info(struct kvaser_usb *dev)\n{\n	struct kvaser_msg msg;\n	int err;\n\n	err = kvaser_usb_send_simple_msg(dev, CMD_GET_SOFTWARE_INFO, 0);\n	if (err)\n		return err;\n\n	err = kvaser_usb_wait_msg(dev, CMD_GET_SOFTWARE_INFO_REPLY, &msg);\n	if (err)\n		return err;\n\n	dev->fw_version = le32_to_cpu(msg.u.softinfo.fw_version);\n\n	return 0;\n}\n\nstatic int kvaser_usb_get_card_info(struct kvaser_usb *dev)\n{\n	struct kvaser_msg msg;\n	int err;\n\n	err = kvaser_usb_send_simple_msg(dev, CMD_GET_CARD_INFO, 0);\n	if (err)\n		return err;\n\n	err = kvaser_usb_wait_msg(dev, CMD_GET_CARD_INFO_REPLY, &msg);\n	if (err)\n		return err;\n\n	dev->nchannels = msg.u.cardinfo.nchannels;\n	if (dev->nchannels > MAX_NET_DEVICES)\n		return -EINVAL;\n\n	return 0;\n}\n\nstatic void kvaser_usb_tx_acknowledge(const struct kvaser_usb *dev,\n				      const struct kvaser_msg *msg)\n{\n	struct net_device_stats *stats;\n	struct kvaser_usb_tx_urb_context *context;\n	struct kvaser_usb_net_priv *priv;\n	struct sk_buff *skb;\n	struct can_frame *cf;\n	u8 channel = msg->u.tx_acknowledge.channel;\n	u8 tid = msg->u.tx_acknowledge.tid;\n\n	if (channel >= dev->nchannels) {\n		dev_err(dev->udev->dev.parent,\n			"Invalid channel number (%d)\n", channel);\n		return;\n	}\n\n	priv = dev->nets[channel];\n\n	if (!netif_device_present(priv->netdev))\n		return;\n\n	stats = &priv->netdev->stats;\n\n	context = &priv->tx_contexts[tid % MAX_TX_URBS];\n\n	/* Sometimes the state change doesn\'t come after a bus-off event */\n	if (priv->can.restart_ms &&\n	    (priv->can.state >= CAN_STATE_BUS_OFF)) {\n		skb = alloc_can_err_skb(priv->netdev, &cf);\n		if (skb) {\n			cf->can_id |= CAN_ERR_RESTARTED;\n			netif_rx(skb);\n\n			stats->rx_packets++;\n			stats->rx_bytes += cf->can_dlc;\n		} else {\n			netdev_err(priv->netdev,\n				   "No memory left for err_skb\n");\n		}\n\n		priv->can.can_stats.restarts++;\n		netif_carrier_on(priv->netdev);\n\n		priv->can.state = CAN_STATE_ERROR_ACTIVE;\n	}\n\n	stats->tx_packets++;\n	stats->tx_bytes += context->dlc;\n	can_get_echo_skb(priv->netdev, context->echo_index);\n\n	context->echo_index = MAX_TX_URBS;\n	atomic_dec(&priv->active_tx_urbs);\n\n	netif_wake_queue(priv->netdev);\n}\n\nstatic void kvaser_usb_simple_msg_callback(struct urb *urb)\n{\n	struct net_device *netdev = urb->context;\n\n	kfree(urb->transfer_buffer);\n\n	if (urb->status)\n		netdev_warn(netdev, "urb status received: %d\n",\n			    urb->status);\n}\n\nstatic int kvaser_usb_simple_msg_async(struct kvaser_usb_net_priv *priv,\n				       u8 msg_id)\n{\n	struct kvaser_usb *dev = priv->dev;\n	struct net_device *netdev = priv->netdev;\n	struct kvaser_msg *msg;\n	struct urb *urb;\n	void *buf;\n	int err;\n\n	urb = usb_alloc_urb(0, GFP_ATOMIC);\n	if (!urb) {\n		netdev_err(netdev, "No memory left for URBs\n");\n		return -ENOMEM;\n	}\n\n	buf = kmalloc(sizeof(struct kvaser_msg), GFP_ATOMIC);\n	if (!buf) {\n		usb_free_urb(urb);\n		return -ENOMEM;\n	}\n\n	msg = (struct kvaser_msg *)buf;\n	msg->len = MSG_HEADER_LEN + sizeof(struct kvaser_msg_simple);\n	msg->id = msg_id;\n	msg->u.simple.channel = priv->channel;\n\n	usb_fill_bulk_urb(urb, dev->udev,\n			  usb_sndbulkpipe(dev->udev,\n					  dev->bulk_out->bEndpointAddress),\n			  buf, msg->len,\n			  kvaser_usb_simple_msg_callback, priv);\n	usb_anchor_urb(urb, &priv->tx_submitted);\n\n	err = usb_submit_urb(urb, GFP_ATOMIC);\n	if (err) {\n		netdev_err(netdev, "Error transmitting URB\n");\n		usb_unanchor_urb(urb);\n		usb_free_urb(urb);\n		kfree(buf);\n		return err;\n	}\n\n	usb_free_urb(urb);\n\n	return 0;\n}\n\nstatic void kvaser_usb_unlink_tx_urbs(struct kvaser_usb_net_priv *priv)\n{\n	int i;\n\n	usb_kill_anchored_urbs(&priv->tx_submitted);\n	atomic_set(&priv->active_tx_urbs, 0);\n\n	for (i = 0; i < MAX_TX_URBS; i++)\n		priv->tx_contexts[i].echo_index = MAX_TX_URBS;\n}\n\nstatic void kvaser_usb_rx_error(const struct kvaser_usb *dev,\n				const struct kvaser_msg *msg)\n{\n	struct can_frame *cf;\n	struct sk_buff *skb;\n	struct net_device_stats *stats;\n	struct kvaser_usb_net_priv *priv;\n	unsigned int new_state;\n	u8 channel, status, txerr, rxerr, error_factor;\n\n	switch (msg->id) {\n	case CMD_CAN_ERROR_EVENT:\n		channel = msg->u.error_event.channel;\n		status =  msg->u.error_event.status;\n		txerr = msg->u.error_event.tx_errors_count;\n		rxerr = msg->u.error_event.rx_errors_count;\n		error_factor = msg->u.error_event.error_factor;\n		break;\n	case CMD_LOG_MESSAGE:\n		channel = msg->u.log_message.channel;\n		status = msg->u.log_message.data[0];\n		txerr = msg->u.log_message.data[2];\n		rxerr = msg->u.log_message.data[3];\n		error_factor = msg->u.log_message.data[1];\n		break;\n	case CMD_CHIP_STATE_EVENT:\n		channel = msg->u.chip_state_event.channel;\n		status =  msg->u.chip_state_event.status;\n		txerr = msg->u.chip_state_event.tx_errors_count;\n		rxerr = msg->u.chip_state_event.rx_errors_count;\n		error_factor = 0;\n		break;\n	default:\n		dev_err(dev->udev->dev.parent, "Invalid msg id (%d)\n",\n			msg->id);\n		return;\n	}\n\n	if (channel >= dev->nchannels) {\n		dev_err(dev->udev->dev.parent,\n			"Invalid channel number (%d)\n", channel);\n		return;\n	}\n\n	priv = dev->nets[channel];\n	stats = &priv->netdev->stats;\n\n	if (status & M16C_STATE_BUS_RESET) {\n		kvaser_usb_unlink_tx_urbs(priv);\n		return;\n	}\n\n	skb = alloc_can_err_skb(priv->netdev, &cf);\n	if (!skb) {\n		stats->rx_dropped++;\n		return;\n	}\n\n	new_state = priv->can.state;\n\n	netdev_dbg(priv->netdev, "Error status: 0x%02x\n", status);\n\n	if (status & M16C_STATE_BUS_OFF) {\n		cf->can_id |= CAN_ERR_BUSOFF;\n\n		priv->can.can_stats.bus_off++;\n		if (!priv->can.restart_ms)\n			kvaser_usb_simple_msg_async(priv, CMD_STOP_CHIP);\n\n		netif_carrier_off(priv->netdev);\n\n		new_state = CAN_STATE_BUS_OFF;\n	} else if (status & M16C_STATE_BUS_PASSIVE) {\n		if (priv->can.state != CAN_STATE_ERROR_PASSIVE) {\n			cf->can_id |= CAN_ERR_CRTL;\n\n			if (txerr || rxerr)\n				cf->data[1] = (txerr > rxerr)\n						? CAN_ERR_CRTL_TX_PASSIVE\n						: CAN_ERR_CRTL_RX_PASSIVE;\n			else\n				cf->data[1] = CAN_ERR_CRTL_TX_PASSIVE |\n					      CAN_ERR_CRTL_RX_PASSIVE;\n\n			priv->can.can_stats.error_passive++;\n		}\n\n		new_state = CAN_STATE_ERROR_PASSIVE;\n	}\n\n	if (status == M16C_STATE_BUS_ERROR) {\n		if ((priv->can.state < CAN_STATE_ERROR_WARNING) &&\n		    ((txerr >= 96) || (rxerr >= 96))) {\n			cf->can_id |= CAN_ERR_CRTL;\n			cf->data[1] = (txerr > rxerr)\n					? CAN_ERR_CRTL_TX_WARNING\n					: CAN_ERR_CRTL_RX_WARNING;\n\n			priv->can.can_stats.error_warning++;\n			new_state = CAN_STATE_ERROR_WARNING;\n		} else if (priv->can.state > CAN_STATE_ERROR_ACTIVE) {\n			cf->can_id |= CAN_ERR_PROT;\n			cf->data[2] = CAN_ERR_PROT_ACTIVE;\n\n			new_state = CAN_STATE_ERROR_ACTIVE;\n		}\n	}\n\n	if (!status) {\n		cf->can_id |= CAN_ERR_PROT;\n		cf->data[2] = CAN_ERR_PROT_ACTIVE;\n\n		new_state = CAN_STATE_ERROR_ACTIVE;\n	}\n\n	if (priv->can.restart_ms &&\n	    (priv->can.state >= CAN_STATE_BUS_OFF) &&\n	    (new_state < CAN_STATE_BUS_OFF)) {\n		cf->can_id |= CAN_ERR_RESTARTED;\n		netif_carrier_on(priv->netdev);\n\n		priv->can.can_stats.restarts++;\n	}\n\n	if (error_factor) {\n		priv->can.can_stats.bus_error++;\n		stats->rx_errors++;\n\n		cf->can_id |= CAN_ERR_BUSERROR | CAN_ERR_PROT;\n\n		if (error_factor & M16C_EF_ACKE)\n			cf->data[3] |= (CAN_ERR_PROT_LOC_ACK);\n		if (error_factor & M16C_EF_CRCE)\n			cf->data[3] |= (CAN_ERR_PROT_LOC_CRC_SEQ |\n					CAN_ERR_PROT_LOC_CRC_DEL);\n		if (error_factor & M16C_EF_FORME)\n			cf->data[2] |= CAN_ERR_PROT_FORM;\n		if (error_factor & M16C_EF_STFE)\n			cf->data[2] |= CAN_ERR_PROT_STUFF;\n		if (error_factor & M16C_EF_BITE0)\n			cf->data[2] |= CAN_ERR_PROT_BIT0;\n		if (error_factor & M16C_EF_BITE1)\n			cf->data[2] |= CAN_ERR_PROT_BIT1;\n		if (error_factor & M16C_EF_TRE)\n			cf->data[2] |= CAN_ERR_PROT_TX;\n	}\n\n	cf->data[6] = txerr;\n	cf->data[7] = rxerr;\n\n	priv->bec.txerr = txerr;\n	priv->bec.rxerr = rxerr;\n\n	priv->can.state = new_state;\n\n	netif_rx(skb);\n\n	stats->rx_packets++;\n	stats->rx_bytes += cf->can_dlc;\n}\n\nstatic void kvaser_usb_rx_can_err(const struct kvaser_usb_net_priv *priv,\n				  const struct kvaser_msg *msg)\n{\n	struct can_frame *cf;\n	struct sk_buff *skb;\n	struct net_device_stats *stats = &priv->netdev->stats;\n\n	if (msg->u.rx_can.flag & (MSG_FLAG_ERROR_FRAME |\n					 MSG_FLAG_NERR)) {\n		netdev_err(priv->netdev, "Unknow error (flags: 0x%02x)\n",\n			   msg->u.rx_can.flag);\n\n		stats->rx_errors++;\n		return;\n	}\n\n	if (msg->u.rx_can.flag & MSG_FLAG_OVERRUN) {\n		skb = alloc_can_err_skb(priv->netdev, &cf);\n		if (!skb) {\n			stats->rx_dropped++;\n			return;\n		}\n\n		cf->can_id |= CAN_ERR_CRTL;\n		cf->data[1] = CAN_ERR_CRTL_RX_OVERFLOW;\n\n		stats->rx_over_errors++;\n		stats->rx_errors++;\n\n		netif_rx(skb);\n\n		stats->rx_packets++;\n		stats->rx_bytes += cf->can_dlc;\n	}\n}\n\nstatic void kvaser_usb_rx_can_msg(const struct kvaser_usb *dev,\n				  const struct kvaser_msg *msg)\n{\n	struct kvaser_usb_net_priv *priv;\n	struct can_frame *cf;\n	struct sk_buff *skb;\n	struct net_device_stats *stats;\n	u8 channel = msg->u.rx_can.channel;\n\n	if (channel >= dev->nchannels) {\n		dev_err(dev->udev->dev.parent,\n			"Invalid channel number (%d)\n", channel);\n		return;\n	}\n\n	priv = dev->nets[channel];\n	stats = &priv->netdev->stats;\n\n	if ((msg->u.rx_can.flag & MSG_FLAG_ERROR_FRAME) &&\n	    (msg->id == CMD_LOG_MESSAGE)) {\n		kvaser_usb_rx_error(dev, msg);\n		return;\n	} else if (msg->u.rx_can.flag & (MSG_FLAG_ERROR_FRAME |\n					 MSG_FLAG_NERR |\n					 MSG_FLAG_OVERRUN)) {\n		kvaser_usb_rx_can_err(priv, msg);\n		return;\n	} else if (msg->u.rx_can.flag & ~MSG_FLAG_REMOTE_FRAME) {\n		netdev_warn(priv->netdev,\n			    "Unhandled frame (flags: 0x%02x)",\n			    msg->u.rx_can.flag);\n		return;\n	}\n\n	skb = alloc_can_skb(priv->netdev, &cf);\n	if (!skb) {\n		stats->tx_dropped++;\n		return;\n	}\n\n	if (msg->id == CMD_LOG_MESSAGE) {\n		cf->can_id = le32_to_cpu(msg->u.log_message.id);\n		if (cf->can_id & KVASER_EXTENDED_FRAME)\n			cf->can_id &= CAN_EFF_MASK | CAN_EFF_FLAG;\n		else\n			cf->can_id &= CAN_SFF_MASK;\n\n		cf->can_dlc = get_can_dlc(msg->u.log_message.dlc);\n\n		if (msg->u.log_message.flags & MSG_FLAG_REMOTE_FRAME)\n			cf->can_id |= CAN_RTR_FLAG;\n		else\n			memcpy(cf->data, &msg->u.log_message.data,\n			       cf->can_dlc);\n	} else {\n		cf->can_id = ((msg->u.rx_can.msg[0] & 0x1f) << 6) |\n			     (msg->u.rx_can.msg[1] & 0x3f);\n\n		if (msg->id == CMD_RX_EXT_MESSAGE) {\n			cf->can_id <<= 18;\n			cf->can_id |= ((msg->u.rx_can.msg[2] & 0x0f) << 14) |\n				      ((msg->u.rx_can.msg[3] & 0xff) << 6) |\n				      (msg->u.rx_can.msg[4] & 0x3f);\n			cf->can_id |= CAN_EFF_FLAG;\n		}\n\n		cf->can_dlc = get_can_dlc(msg->u.rx_can.msg[5]);\n\n		if (msg->u.rx_can.flag & MSG_FLAG_REMOTE_FRAME)\n			cf->can_id |= CAN_RTR_FLAG;\n		else\n			memcpy(cf->data, &msg->u.rx_can.msg[6],\n			       cf->can_dlc);\n	}\n\n	netif_rx(skb);\n\n	stats->rx_packets++;\n	stats->rx_bytes += cf->can_dlc;\n}\n\nstatic void kvaser_usb_start_chip_reply(const struct kvaser_usb *dev,\n					const struct kvaser_msg *msg)\n{\n	struct kvaser_usb_net_priv *priv;\n	u8 channel = msg->u.simple.channel;\n\n	if (channel >= dev->nchannels) {\n		dev_err(dev->udev->dev.parent,\n			"Invalid channel number (%d)\n", channel);\n		return;\n	}\n\n	priv = dev->nets[channel];\n\n	if (completion_done(&priv->start_comp) &&\n	    netif_queue_stopped(priv->netdev)) {\n		netif_wake_queue(priv->netdev);\n	} else {\n		netif_start_queue(priv->netdev);\n		complete(&priv->start_comp);\n	}\n}\n\nstatic void kvaser_usb_stop_chip_reply(const struct kvaser_usb *dev,\n				       const struct kvaser_msg *msg)\n{\n	struct kvaser_usb_net_priv *priv;\n	u8 channel = msg->u.simple.channel;\n\n	if (channel >= dev->nchannels) {\n		dev_err(dev->udev->dev.parent,\n			"Invalid channel number (%d)\n", channel);\n		return;\n	}\n\n	priv = dev->nets[channel];\n\n	complete(&priv->stop_comp);\n}\n\nstatic void kvaser_usb_handle_message(const struct kvaser_usb *dev,\n				      const struct kvaser_msg *msg)\n{\n	switch (msg->id) {\n	case CMD_START_CHIP_REPLY:\n		kvaser_usb_start_chip_reply(dev, msg);\n		break;\n\n	case CMD_STOP_CHIP_REPLY:\n		kvaser_usb_stop_chip_reply(dev, msg);\n		break;\n\n	case CMD_RX_STD_MESSAGE:\n	case CMD_RX_EXT_MESSAGE:\n	case CMD_LOG_MESSAGE:\n		kvaser_usb_rx_can_msg(dev, msg);\n		break;\n\n	case CMD_CHIP_STATE_EVENT:\n	case CMD_CAN_ERROR_EVENT:\n		kvaser_usb_rx_error(dev, msg);\n		break;\n\n	case CMD_TX_ACKNOWLEDGE:\n		kvaser_usb_tx_acknowledge(dev, msg);\n		break;\n\n	default:\n		dev_warn(dev->udev->dev.parent,\n			 "Unhandled message (%d)\n", msg->id);\n		break;\n	}\n}\n\nstatic void kvaser_usb_read_bulk_callback(struct urb *urb)\n{\n	struct kvaser_usb *dev = urb->context;\n	struct kvaser_msg *msg;\n	int pos = 0;\n	int err, i;\n\n	switch (urb->status) {\n	case 0:\n		break;\n	case -ENOENT:\n	case -ESHUTDOWN:\n		return;\n	default:\n		dev_info(dev->udev->dev.parent, "Rx URB aborted (%d)\n",\n			 urb->status);\n		goto resubmit_urb;\n	}\n\n	while (pos <= urb->actual_length - MSG_HEADER_LEN) {\n		msg = urb->transfer_buffer + pos;\n\n		if (!msg->len)\n			break;\n\n		if (pos + msg->len > urb->actual_length) {\n			dev_err(dev->udev->dev.parent, "Format error\n");\n			break;\n		}\n\n		kvaser_usb_handle_message(dev, msg);\n\n		pos += msg->len;\n	}\n\nresubmit_urb:\n	usb_fill_bulk_urb(urb, dev->udev,\n			  usb_rcvbulkpipe(dev->udev,\n					  dev->bulk_in->bEndpointAddress),\n			  urb->transfer_buffer, RX_BUFFER_SIZE,\n			  kvaser_usb_read_bulk_callback, dev);\n\n	err = usb_submit_urb(urb, GFP_ATOMIC);\n	if (err == -ENODEV) {\n		for (i = 0; i < dev->nchannels; i++) {\n			if (!dev->nets[i])\n				continue;\n\n			netif_device_detach(dev->nets[i]->netdev);\n		}\n	} else if (err) {\n		dev_err(dev->udev->dev.parent,\n			"Failed resubmitting read bulk urb: %d\n", err);\n	}\n\n	return;\n}\n\nstatic int kvaser_usb_setup_rx_urbs(struct kvaser_usb *dev)\n{\n	int i, err = 0;\n\n	if (dev->rxinitdone)\n		return 0;\n\n	for (i = 0; i < MAX_RX_URBS; i++) {\n		struct urb *urb = NULL;\n		u8 *buf = NULL;\n		dma_addr_t buf_dma;\n\n		urb = usb_alloc_urb(0, GFP_KERNEL);\n		if (!urb) {\n			dev_warn(dev->udev->dev.parent,\n				 "No memory left for URBs\n");\n			err = -ENOMEM;\n			break;\n		}\n\n		buf = usb_alloc_coherent(dev->udev, RX_BUFFER_SIZE,\n					 GFP_KERNEL, &buf_dma);\n		if (!buf) {\n			dev_warn(dev->udev->dev.parent,\n				 "No memory left for USB buffer\n");\n			usb_free_urb(urb);\n			err = -ENOMEM;\n			break;\n		}\n\n		usb_fill_bulk_urb(urb, dev->udev,\n				  usb_rcvbulkpipe(dev->udev,\n					  dev->bulk_in->bEndpointAddress),\n				  buf, RX_BUFFER_SIZE,\n				  kvaser_usb_read_bulk_callback,\n				  dev);\n		urb->transfer_dma = buf_dma;\n		urb->transfer_flags |= URB_NO_TRANSFER_DMA_MAP;\n		usb_anchor_urb(urb, &dev->rx_submitted);\n\n		err = usb_submit_urb(urb, GFP_KERNEL);\n		if (err) {\n			usb_unanchor_urb(urb);\n			usb_free_coherent(dev->udev, RX_BUFFER_SIZE, buf,\n					  buf_dma);\n			usb_free_urb(urb);\n			break;\n		}\n\n		dev->rxbuf[i] = buf;\n		dev->rxbuf_dma[i] = buf_dma;\n\n		usb_free_urb(urb);\n	}\n\n	if (i == 0) {\n		dev_warn(dev->udev->dev.parent,\n			 "Cannot setup read URBs, error %d\n", err);\n		return err;\n	} else if (i < MAX_RX_URBS) {\n		dev_warn(dev->udev->dev.parent,\n			 "RX performances may be slow\n");\n	}\n\n	dev->rxinitdone = true;\n\n	return 0;\n}\n\nstatic int kvaser_usb_set_opt_mode(const struct kvaser_usb_net_priv *priv)\n{\n	struct kvaser_msg *msg;\n	int rc;\n\n	msg = kmalloc(sizeof(*msg), GFP_KERNEL);\n	if (!msg)\n		return -ENOMEM;\n\n	msg->id = CMD_SET_CTRL_MODE;\n	msg->len = MSG_HEADER_LEN + sizeof(struct kvaser_msg_ctrl_mode);\n	msg->u.ctrl_mode.tid = 0xff;\n	msg->u.ctrl_mode.channel = priv->channel;\n\n	if (priv->can.ctrlmode & CAN_CTRLMODE_LISTENONLY)\n		msg->u.ctrl_mode.ctrl_mode = KVASER_CTRL_MODE_SILENT;\n	else\n		msg->u.ctrl_mode.ctrl_mode = KVASER_CTRL_MODE_NORMAL;\n\n	rc = kvaser_usb_send_msg(priv->dev, msg);\n\n	kfree(msg);\n	return rc;\n}\n\nstatic int kvaser_usb_start_chip(struct kvaser_usb_net_priv *priv)\n{\n	int err;\n\n	init_completion(&priv->start_comp);\n\n	err = kvaser_usb_send_simple_msg(priv->dev, CMD_START_CHIP,\n					 priv->channel);\n	if (err)\n		return err;\n\n	if (!wait_for_completion_timeout(&priv->start_comp,\n					 msecs_to_jiffies(START_TIMEOUT)))\n		return -ETIMEDOUT;\n\n	return 0;\n}\n\nstatic int kvaser_usb_open(struct net_device *netdev)\n{\n	struct kvaser_usb_net_priv *priv = netdev_priv(netdev);\n	struct kvaser_usb *dev = priv->dev;\n	int err;\n\n	err = open_candev(netdev);\n	if (err)\n		return err;\n\n	err = kvaser_usb_setup_rx_urbs(dev);\n	if (err)\n		goto error;\n\n	err = kvaser_usb_set_opt_mode(priv);\n	if (err)\n		goto error;\n\n	err = kvaser_usb_start_chip(priv);\n	if (err) {\n		netdev_warn(netdev, "Cannot start device, error %d\n", err);\n		goto error;\n	}\n\n	priv->can.state = CAN_STATE_ERROR_ACTIVE;\n\n	return 0;\n\nerror:\n	close_candev(netdev);\n	return err;\n}\n\nstatic void kvaser_usb_unlink_all_urbs(struct kvaser_usb *dev)\n{\n	int i;\n\n	usb_kill_anchored_urbs(&dev->rx_submitted);\n\n	for (i = 0; i < MAX_RX_URBS; i++)\n		usb_free_coherent(dev->udev, RX_BUFFER_SIZE,\n				  dev->rxbuf[i],\n				  dev->rxbuf_dma[i]);\n\n	for (i = 0; i < MAX_NET_DEVICES; i++) {\n		struct kvaser_usb_net_priv *priv = dev->nets[i];\n\n		if (priv)\n			kvaser_usb_unlink_tx_urbs(priv);\n	}\n}\n\nstatic int kvaser_usb_stop_chip(struct kvaser_usb_net_priv *priv)\n{\n	int err;\n\n	init_completion(&priv->stop_comp);\n\n	err = kvaser_usb_send_simple_msg(priv->dev, CMD_STOP_CHIP,\n					 priv->channel);\n	if (err)\n		return err;\n\n	if (!wait_for_completion_timeout(&priv->stop_comp,\n					 msecs_to_jiffies(STOP_TIMEOUT)))\n		return -ETIMEDOUT;\n\n	return 0;\n}\n\nstatic int kvaser_usb_flush_queue(struct kvaser_usb_net_priv *priv)\n{\n	struct kvaser_msg *msg;\n	int rc;\n\n	msg = kmalloc(sizeof(*msg), GFP_KERNEL);\n	if (!msg)\n		return -ENOMEM;\n\n	msg->id = CMD_FLUSH_QUEUE;\n	msg->len = MSG_HEADER_LEN + sizeof(struct kvaser_msg_flush_queue);\n	msg->u.flush_queue.channel = priv->channel;\n	msg->u.flush_queue.flags = 0x00;\n\n	rc = kvaser_usb_send_msg(priv->dev, msg);\n\n	kfree(msg);\n	return rc;\n}\n\nstatic int kvaser_usb_close(struct net_device *netdev)\n{\n	struct kvaser_usb_net_priv *priv = netdev_priv(netdev);\n	struct kvaser_usb *dev = priv->dev;\n	int err;\n\n	netif_stop_queue(netdev);\n\n	err = kvaser_usb_flush_queue(priv);\n	if (err)\n		netdev_warn(netdev, "Cannot flush queue, error %d\n", err);\n\n	if (kvaser_usb_send_simple_msg(dev, CMD_RESET_CHIP, priv->channel))\n		netdev_warn(netdev, "Cannot reset card, error %d\n", err);\n\n	err = kvaser_usb_stop_chip(priv);\n	if (err)\n		netdev_warn(netdev, "Cannot stop device, error %d\n", err);\n\n	priv->can.state = CAN_STATE_STOPPED;\n	close_candev(priv->netdev);\n\n	return 0;\n}\n\nstatic void kvaser_usb_write_bulk_callback(struct urb *urb)\n{\n	struct kvaser_usb_tx_urb_context *context = urb->context;\n	struct kvaser_usb_net_priv *priv;\n	struct net_device *netdev;\n\n	if (WARN_ON(!context))\n		return;\n\n	priv = context->priv;\n	netdev = priv->netdev;\n\n	kfree(urb->transfer_buffer);\n\n	if (!netif_device_present(netdev))\n		return;\n\n	if (urb->status)\n		netdev_info(netdev, "Tx URB aborted (%d)\n", urb->status);\n}\n\nstatic netdev_tx_t kvaser_usb_start_xmit(struct sk_buff *skb,\n					 struct net_device *netdev)\n{\n	struct kvaser_usb_net_priv *priv = netdev_priv(netdev);\n	struct kvaser_usb *dev = priv->dev;\n	struct net_device_stats *stats = &netdev->stats;\n	struct can_frame *cf = (struct can_frame *)skb->data;\n	struct kvaser_usb_tx_urb_context *context = NULL;\n	struct urb *urb;\n	void *buf;\n	struct kvaser_msg *msg;\n	int i, err;\n	int ret = NETDEV_TX_OK;\n\n	if (can_dropped_invalid_skb(netdev, skb))\n		return NETDEV_TX_OK;\n\n	urb = usb_alloc_urb(0, GFP_ATOMIC);\n	if (!urb) {\n		netdev_err(netdev, "No memory left for URBs\n");\n		stats->tx_dropped++;\n		goto nourbmem;\n	}\n\n	buf = kmalloc(sizeof(struct kvaser_msg), GFP_ATOMIC);\n	if (!buf) {\n		stats->tx_dropped++;\n		goto nobufmem;\n	}\n\n	msg = buf;\n	msg->len = MSG_HEADER_LEN + sizeof(struct kvaser_msg_tx_can);\n	msg->u.tx_can.flags = 0;\n	msg->u.tx_can.channel = priv->channel;\n\n	if (cf->can_id & CAN_EFF_FLAG) {\n		msg->id = CMD_TX_EXT_MESSAGE;\n		msg->u.tx_can.msg[0] = (cf->can_id >> 24) & 0x1f;\n		msg->u.tx_can.msg[1] = (cf->can_id >> 18) & 0x3f;\n		msg->u.tx_can.msg[2] = (cf->can_id >> 14) & 0x0f;\n		msg->u.tx_can.msg[3] = (cf->can_id >> 6) & 0xff;\n		msg->u.tx_can.msg[4] = cf->can_id & 0x3f;\n	} else {\n		msg->id = CMD_TX_STD_MESSAGE;\n		msg->u.tx_can.msg[0] = (cf->can_id >> 6) & 0x1f;\n		msg->u.tx_can.msg[1] = cf->can_id & 0x3f;\n	}\n\n	msg->u.tx_can.msg[5] = cf->can_dlc;\n	memcpy(&msg->u.tx_can.msg[6], cf->data, cf->can_dlc);\n\n	if (cf->can_id & CAN_RTR_FLAG)\n		msg->u.tx_can.flags |= MSG_FLAG_REMOTE_FRAME;\n\n	for (i = 0; i < ARRAY_SIZE(priv->tx_contexts); i++) {\n		if (priv->tx_contexts[i].echo_index == MAX_TX_URBS) {\n			context = &priv->tx_contexts[i];\n			break;\n		}\n	}\n\n	if (!context) {\n		netdev_warn(netdev, "cannot find free context\n");\n		ret =  NETDEV_TX_BUSY;\n		goto releasebuf;\n	}\n\n	context->priv = priv;\n	context->echo_index = i;\n	context->dlc = cf->can_dlc;\n\n	msg->u.tx_can.tid = context->echo_index;\n\n	usb_fill_bulk_urb(urb, dev->udev,\n			  usb_sndbulkpipe(dev->udev,\n					  dev->bulk_out->bEndpointAddress),\n			  buf, msg->len,\n			  kvaser_usb_write_bulk_callback, context);\n	usb_anchor_urb(urb, &priv->tx_submitted);\n\n	can_put_echo_skb(skb, netdev, context->echo_index);\n\n	atomic_inc(&priv->active_tx_urbs);\n\n	if (atomic_read(&priv->active_tx_urbs) >= MAX_TX_URBS)\n		netif_stop_queue(netdev);\n\n	err = usb_submit_urb(urb, GFP_ATOMIC);\n	if (unlikely(err)) {\n		can_free_echo_skb(netdev, context->echo_index);\n\n		skb = NULL; /* set to NULL to avoid double free in\n			     * dev_kfree_skb(skb) */\n\n		atomic_dec(&priv->active_tx_urbs);\n		usb_unanchor_urb(urb);\n\n		stats->tx_dropped++;\n\n		if (err == -ENODEV)\n			netif_device_detach(netdev);\n		else\n			netdev_warn(netdev, "Failed tx_urb %d\n", err);\n\n		goto releasebuf;\n	}\n\n	usb_free_urb(urb);\n\n	return NETDEV_TX_OK;\n\nreleasebuf:\n	kfree(buf);\nnobufmem:\n	usb_free_urb(urb);\nnourbmem:\n	dev_kfree_skb(skb);\n	return ret;\n}\n\nstatic const struct net_device_ops kvaser_usb_netdev_ops = {\n	.ndo_open = kvaser_usb_open,\n	.ndo_stop = kvaser_usb_close,\n	.ndo_start_xmit = kvaser_usb_start_xmit,\n	.ndo_change_mtu = can_change_mtu,\n};\n\nstatic const struct can_bittiming_const kvaser_usb_bittiming_const = {\n	.name = "kvaser_usb",\n	.tseg1_min = KVASER_USB_TSEG1_MIN,\n	.tseg1_max = KVASER_USB_TSEG1_MAX,\n	.tseg2_min = KVASER_USB_TSEG2_MIN,\n	.tseg2_max = KVASER_USB_TSEG2_MAX,\n	.sjw_max = KVASER_USB_SJW_MAX,\n	.brp_min = KVASER_USB_BRP_MIN,\n	.brp_max = KVASER_USB_BRP_MAX,\n	.brp_inc = KVASER_USB_BRP_INC,\n};\n\nstatic int kvaser_usb_set_bittiming(struct net_device *netdev)\n{\n	struct kvaser_usb_net_priv *priv = netdev_priv(netdev);\n	struct can_bittiming *bt = &priv->can.bittiming;\n	struct kvaser_usb *dev = priv->dev;\n	struct kvaser_msg *msg;\n	int rc;\n\n	msg = kmalloc(sizeof(*msg), GFP_KERNEL);\n	if (!msg)\n		return -ENOMEM;\n\n	msg->id = CMD_SET_BUS_PARAMS;\n	msg->len = MSG_HEADER_LEN + sizeof(struct kvaser_msg_busparams);\n	msg->u.busparams.channel = priv->channel;\n	msg->u.busparams.tid = 0xff;\n	msg->u.busparams.bitrate = cpu_to_le32(bt->bitrate);\n	msg->u.busparams.sjw = bt->sjw;\n	msg->u.busparams.tseg1 = bt->prop_seg + bt->phase_seg1;\n	msg->u.busparams.tseg2 = bt->phase_seg2;\n\n	if (priv->can.ctrlmode & CAN_CTRLMODE_3_SAMPLES)\n		msg->u.busparams.no_samp = 3;\n	else\n		msg->u.busparams.no_samp = 1;\n\n	rc = kvaser_usb_send_msg(dev, msg);\n\n	kfree(msg);\n	return rc;\n}\n\nstatic int kvaser_usb_set_mode(struct net_device *netdev,\n			       enum can_mode mode)\n{\n	struct kvaser_usb_net_priv *priv = netdev_priv(netdev);\n	int err;\n\n	switch (mode) {\n	case CAN_MODE_START:\n		err = kvaser_usb_simple_msg_async(priv, CMD_START_CHIP);\n		if (err)\n			return err;\n		break;\n	default:\n		return -EOPNOTSUPP;\n	}\n\n	return 0;\n}\n\nstatic int kvaser_usb_get_berr_counter(const struct net_device *netdev,\n				       struct can_berr_counter *bec)\n{\n	struct kvaser_usb_net_priv *priv = netdev_priv(netdev);\n\n	*bec = priv->bec;\n\n	return 0;\n}\n\nstatic void kvaser_usb_remove_interfaces(struct kvaser_usb *dev)\n{\n	int i;\n\n	for (i = 0; i < dev->nchannels; i++) {\n		if (!dev->nets[i])\n			continue;\n\n		unregister_netdev(dev->nets[i]->netdev);\n	}\n\n	kvaser_usb_unlink_all_urbs(dev);\n\n	for (i = 0; i < dev->nchannels; i++) {\n		if (!dev->nets[i])\n			continue;\n\n		free_candev(dev->nets[i]->netdev);\n	}\n}\n\nstatic int kvaser_usb_init_one(struct usb_interface *intf,\n			       const struct usb_device_id *id, int channel)\n{\n	struct kvaser_usb *dev = usb_get_intfdata(intf);\n	struct net_device *netdev;\n	struct kvaser_usb_net_priv *priv;\n	int i, err;\n\n	netdev = alloc_candev(sizeof(*priv), MAX_TX_URBS);\n	if (!netdev) {\n		dev_err(&intf->dev, "Cannot alloc candev\n");\n		return -ENOMEM;\n	}\n\n	priv = netdev_priv(netdev);\n\n	init_completion(&priv->start_comp);\n	init_completion(&priv->stop_comp);\n\n	init_usb_anchor(&priv->tx_submitted);\n	atomic_set(&priv->active_tx_urbs, 0);\n\n	for (i = 0; i < ARRAY_SIZE(priv->tx_contexts); i++)\n		priv->tx_contexts[i].echo_index = MAX_TX_URBS;\n\n	priv->dev = dev;\n	priv->netdev = netdev;\n	priv->channel = channel;\n\n	priv->can.state = CAN_STATE_STOPPED;\n	priv->can.clock.freq = CAN_USB_CLOCK;\n	priv->can.bittiming_const = &kvaser_usb_bittiming_const;\n	priv->can.do_set_bittiming = kvaser_usb_set_bittiming;\n	priv->can.do_set_mode = kvaser_usb_set_mode;\n	if (id->driver_info & KVASER_HAS_TXRX_ERRORS)\n		priv->can.do_get_berr_counter = kvaser_usb_get_berr_counter;\n	priv->can.ctrlmode_supported = CAN_CTRLMODE_3_SAMPLES;\n	if (id->driver_info & KVASER_HAS_SILENT_MODE)\n		priv->can.ctrlmode_supported |= CAN_CTRLMODE_LISTENONLY;\n\n	netdev->flags |= IFF_ECHO;\n\n	netdev->netdev_ops = &kvaser_usb_netdev_ops;\n\n	SET_NETDEV_DEV(netdev, &intf->dev);\n	netdev->dev_id = channel;\n\n	dev->nets[channel] = priv;\n\n	err = register_candev(netdev);\n	if (err) {\n		dev_err(&intf->dev, "Failed to register can device\n");\n		free_candev(netdev);\n		dev->nets[channel] = NULL;\n		return err;\n	}\n\n	netdev_dbg(netdev, "device registered\n");\n\n	return 0;\n}\n\nstatic int kvaser_usb_get_endpoints(const struct usb_interface *intf,\n				    struct usb_endpoint_descriptor **in,\n				    struct usb_endpoint_descriptor **out)\n{\n	const struct usb_host_interface *iface_desc;\n	struct usb_endpoint_descriptor *endpoint;\n	int i;\n\n	iface_desc = &intf->altsetting[0];\n\n	for (i = 0; i < iface_desc->desc.bNumEndpoints; ++i) {\n		endpoint = &iface_desc->endpoint[i].desc;\n\n		if (!*in && usb_endpoint_is_bulk_in(endpoint))\n			*in = endpoint;\n\n		if (!*out && usb_endpoint_is_bulk_out(endpoint))\n			*out = endpoint;\n\n		/* use first bulk endpoint for in and out */\n		if (*in && *out)\n			return 0;\n	}\n\n	return -ENODEV;\n}\n\nstatic int kvaser_usb_probe(struct usb_interface *intf,\n			    const struct usb_device_id *id)\n{\n	struct kvaser_usb *dev;\n	int err = -ENOMEM;\n	int i;\n\n	dev = devm_kzalloc(&intf->dev, sizeof(*dev), GFP_KERNEL);\n	if (!dev)\n		return -ENOMEM;\n\n	err = kvaser_usb_get_endpoints(intf, &dev->bulk_in, &dev->bulk_out);\n	if (err) {\n		dev_err(&intf->dev, "Cannot get usb endpoint(s)");\n		return err;\n	}\n\n	dev->udev = interface_to_usbdev(intf);\n\n	init_usb_anchor(&dev->rx_submitted);\n\n	usb_set_intfdata(intf, dev);\n\n	for (i = 0; i < MAX_NET_DEVICES; i++)\n		kvaser_usb_send_simple_msg(dev, CMD_RESET_CHIP, i);\n\n	err = kvaser_usb_get_software_info(dev);\n	if (err) {\n		dev_err(&intf->dev,\n			"Cannot get software infos, error %d\n", err);\n		return err;\n	}\n\n	err = kvaser_usb_get_card_info(dev);\n	if (err) {\n		dev_err(&intf->dev,\n			"Cannot get card infos, error %d\n", err);\n		return err;\n	}\n\n	dev_dbg(&intf->dev, "Firmware version: %d.%d.%d\n",\n		((dev->fw_version >> 24) & 0xff),\n		((dev->fw_version >> 16) & 0xff),\n		(dev->fw_version & 0xffff));\n\n	for (i = 0; i < dev->nchannels; i++) {\n		err = kvaser_usb_init_one(intf, id, i);\n		if (err) {\n			kvaser_usb_remove_interfaces(dev);\n			return err;\n		}\n	}\n\n	return 0;\n}\n\nstatic void kvaser_usb_disconnect(struct usb_interface *intf)\n{\n	struct kvaser_usb *dev = usb_get_intfdata(intf);\n\n	usb_set_intfdata(intf, NULL);\n\n	if (!dev)\n		return;\n\n	kvaser_usb_remove_interfaces(dev);\n}\n\nstatic struct usb_driver kvaser_usb_driver = {\n	.name = "kvaser_usb",\n	.probe = kvaser_usb_probe,\n	.disconnect = kvaser_usb_disconnect,\n	.id_table = kvaser_usb_table,\n};\n\nstatic inline struct ucma_context *_ucma_find_context(int id,\n						      struct ucma_file *file)\n{\n	struct ucma_context *ctx;\n\n	ctx = idr_find(&ctx_idr, id);\n	if (!ctx)\n		ctx = ERR_PTR(-ENOENT);\n	else if (ctx->file != file)\n		ctx = ERR_PTR(-EINVAL);\n	return ctx;\n}\n\nstatic struct ucma_context *ucma_get_ctx(struct ucma_file *file, int id)\n{\n	struct ucma_context *ctx;\n\n	mutex_lock(&mut);\n	ctx = _ucma_find_context(id, file);\n	if (!IS_ERR(ctx))\n		atomic_inc(&ctx->ref);\n	mutex_unlock(&mut);\n	return ctx;\n}